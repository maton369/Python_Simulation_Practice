# ------------------------------------------------------------
# KFold によるホールドアウト分割のデモである
# ・データ配列を K=5 分割し、各分割で学習用/評価用インデックスを返す
# ・shuffle=True, random_state=1 により再現可能なランダム分割である
# ------------------------------------------------------------
import numpy as np
from sklearn.model_selection import KFold

# ------------------------------------------------------------
# 1. 入力データの用意（ここでは単純な整数列）である
#    10 から 100 まで 10 刻みの配列（要素数 10）を作成する
# ------------------------------------------------------------
StartedData = np.arange(10, 110, 10)
print("StartedData =", StartedData)  # -> [ 10  20  30  40  50  60  70  80  90 100]

# ------------------------------------------------------------
# 2. KFold 分割器の設定である
#    n_splits=5 : 5-分割交差検証
#    shuffle=True : 分割前にデータをシャッフルする（順序の偏りを避けるためである）
#    random_state=1 : 乱数シード固定で再現性を確保する
# ------------------------------------------------------------
kfold = KFold(n_splits=5, shuffle=True, random_state=1)

# ------------------------------------------------------------
# 3. 各分割の学習/評価インデックスと実データを出力するである
#    kfold.split(X) は (train_index, test_index) のタプルを各分割ごとに返す
# ------------------------------------------------------------
for fold, (train_idx, test_idx) in enumerate(kfold.split(StartedData), start=1):
    train_data = StartedData[train_idx]
    test_data = StartedData[test_idx]
    print(f"[Fold {fold}]")
    print("  Train indices:", train_idx, "->", train_data)
    print("  Test  indices:", test_idx, "->", test_data)

# ------------------------------------------------------------
# 補足である：
# ・各要素は 1 回だけ test に現れ、残りの分割では train に現れる（KFold の性質である）
# ・分類/回帰の前処理として、このインデックスで特徴量・目的変数を分割して学習・評価する
# ------------------------------------------------------------
