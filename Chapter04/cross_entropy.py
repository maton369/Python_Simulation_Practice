# ------------------------------------------------------------
# クロスエントロピー（Cross Entropy）の計算と可視化
# ------------------------------------------------------------
from matplotlib import pyplot as plt
from math import log2

# ------------------------------------------------------------
# 1. 事象と確率分布の定義
# ------------------------------------------------------------
events = ["A", "B", "C", "D"]
p = [0.70, 0.05, 0.10, 0.15]  # 真の分布 P
q = [0.45, 0.10, 0.20, 0.25]  # モデル分布 Q

# P, Q の正規化確認
print(f"P = {sum(p):.3f}", f"Q = {sum(q):.3f}")

# ------------------------------------------------------------
# 2. 棒グラフで確率分布を可視化
# ------------------------------------------------------------
plt.subplot(2, 1, 1)
plt.bar(events, p, color="skyblue")
plt.title("True Distribution P")

plt.subplot(2, 1, 2)
plt.bar(events, q, color="salmon")
plt.title("Model Distribution Q")

plt.tight_layout()
plt.show()


# ------------------------------------------------------------
# 3. クロスエントロピーの定義
# ------------------------------------------------------------
def cross_entropy(p, q):
    # 各項目ごとに P(x)*log2(Q(x)) を計算し、符号を反転
    # log2(Q(x)) が負なので、結果は正の値（ビット単位）になる
    return -sum([p_i * log2(q_i) for p_i, q_i in zip(p, q)])


# ------------------------------------------------------------
# 4. クロスエントロピーの計算
# ------------------------------------------------------------
h_pq = cross_entropy(p, q)
print(f"H(P, Q) = {h_pq:.3f} bits")

# ------------------------------------------------------------
# 5. 理論的な解釈
# ------------------------------------------------------------
# H(P, Q) = -Σ P(x) log2 Q(x)
#   → 真の分布 P に対して、モデル Q がどれだけ効率よく符号化できるかを表す。
#   → 値が大きいほど、モデル Q は P から乖離している（誤差が大きい）。
#   → 一致すれば H(P, Q) = H(P)（自己エントロピー）となる。
