# ------------------------------------------------------------
# クロスエントロピー損失（Cross Entropy Loss）の計算例（二値分類）
# ------------------------------------------------------------
import numpy as np

# ------------------------------------------------------------
# 1. 真のラベル（y）と予測確率（p）
# ------------------------------------------------------------
# y: 正解ラベル（1 または 0）
# p: モデルが出力した「クラス1である確率」
y = np.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0])
p = np.array([0.8, 0.1, 0.9, 0.2, 0.8, 0.1, 0.7, 0.3, 0.6, 0.4])

# ------------------------------------------------------------
# 2. クロスエントロピー損失の定義
# ------------------------------------------------------------
# 式: L = - (1/N) * Σ [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]
# 単位: nats（自然対数を使用するため）
ce_loss = -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p)) / len(p)

# ------------------------------------------------------------
# 3. 結果の表示
# ------------------------------------------------------------
print(f"Cross Entropy Loss = {ce_loss:.3f} nats")

# ------------------------------------------------------------
# 4. 理論的な補足
# ------------------------------------------------------------
# ・この損失関数は「確率分布 P（真のラベル）」と
#   「モデルの出力確率分布 Q（予測）」の差異を測る。
# ・y=1 の場合は log(p)、y=0 の場合は log(1-p) を評価。
# ・p が正解クラスに近いほど損失が小さくなる。
# ・自然対数 (log) を用いる場合は単位が “nats”。
#   log2 を用いれば “bits” 単位の情報量となる。
