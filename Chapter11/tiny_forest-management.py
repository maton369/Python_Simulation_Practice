# ------------------------------------------------------------
# OpenAI Gym 風の「森林管理問題（Forest MDP）」を題材に、
# PyMDPtoolbox（mdptoolbox）の Policy Iteration（方策反復法）で
# 最適価値関数 V* と最適方策 π* を求める最小例である。
#
# ・mdptoolbox.example.forest() は遷移確率 P と報酬 R を返す。
#   - 典型設定では状態数 S=3、行動は {0: 待機(Wait), 1: 伐採(Cut)} の2種類である。
#   - 山火事などの確率的事象により若齢→壮齢→老齢の推移や伐採・リセットが表現される。
# ・P の形状は A 個（行動数）の (S×S) 行列集合、R は (S×A) の報酬行列である。
# ・割引率 γ（0<γ<1）を与え、方策反復法でベルマン最適性を満たす解を得る。
#
# 参考：方策反復法は「(1) 方策評価 → (2) 方策改善」を収束まで繰り返す反復法であり、
# 価値反復法に比べて反復回数が少ないことが多い一方、各ステップで方策評価を解く必要がある。
# ------------------------------------------------------------

import mdptoolbox  # 方策反復クラス（mdptoolbox.mdp.PolicyIteration）にアクセスするため
import mdptoolbox.example  # 例題（Forest MDP）の P, R を生成するため

# ------------------------------------------------------------
# 例題の遷移確率 P と報酬 R を生成する。
# 既定（S=3, p=0.1, r1=4, r2=2 など）のパラメータで 2 行動の MDP が得られる。
#   P: タプル長 A（=2）。各要素は形状 (S, S) の遷移確率行列で、P[a][i, j] = Pr(s_{t+1}=j | s_t=i, a_t=a)
#   R: 形状 (S, A) の報酬行列で、R[i, a] = 報酬 r(s=i, a)
# ------------------------------------------------------------
P, R = mdptoolbox.example.forest()

# 行動 a=0（待機）の遷移行列、a=1（伐採）の遷移行列を確認するである
print(P[0])
print(P[1])

# 各状態における行動 a=0 / a=1 の即時報酬ベクトルを確認するである
print(R[:, 0])
print(R[:, 1])

# ------------------------------------------------------------
# 割引率 γ を設定する。将来報酬の現在価値重みである。
# γ=0.9 は「1 ステップ先で 90% の価値」を意味する典型的な設定である。
# ------------------------------------------------------------
gamma = 0.9

# ------------------------------------------------------------
# 方策反復モデルを生成する。
#   PolicyIteration(P, R, gamma):
#     ・与えられた MDP（P, R, γ）に対して初期方策から開始
#     ・(評価) 現在の方策の価値関数 V^π を解く
#     ・(改善) 各状態で貪欲に方策を改善（ベルマン最適性条件を満たすまで繰返し）
# 収束すると最適価値関数 V* と最適方策 π* が得られる。
# ------------------------------------------------------------
PolIterModel = mdptoolbox.mdp.PolicyIteration(P, R, gamma)

# 反復を実行する。内部で方策評価・方策改善を収束まで繰り返すである
PolIterModel.run()

# ------------------------------------------------------------
# 結果の出力：
#   V: 各状態の最適価値 V*(s)
#   policy: 各状態の最適行動 π*(s)（Forest 例では 0=待機(Wait), 1=伐採(Cut)）
#   iter: 収束までの外側反復回数
#   time: 実行時間（秒）
# ------------------------------------------------------------
print(PolIterModel.V)
print(PolIterModel.policy)
print(PolIterModel.iter)
print(PolIterModel.time)
